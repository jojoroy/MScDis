{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP-MH implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm as norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a generic MCMC for reference. Later we sort of implemented the MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Target distribution: standard normal distribution\n",
    "def target_distribution(x):\n",
    "    return np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings(target_dist, proposal_dist, proposal_sampler, initial_value, n_samples):\n",
    "    samples = [initial_value]\n",
    "    current_value = initial_value\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        proposed_value = proposal_sampler(current_value)\n",
    "        acceptance_ratio = target_dist(proposed_value) / target_dist(current_value)\n",
    "        \n",
    "        if np.random.rand() < acceptance_ratio:\n",
    "            current_value = proposed_value\n",
    "        \n",
    "        samples.append(current_value)\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# Proposal distribution: symmetric normal distribution centered at the current state\n",
    "def proposal_distribution(x, proposal_width=1.0):\n",
    "    return np.random.normal(x, proposal_width)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "initial_value = 0\n",
    "n_samples = 10000\n",
    "\n",
    "# Run the Metropolis-Hastings algorithm\n",
    "samples = metropolis_hastings(\n",
    "    target_dist=target_distribution,\n",
    "    proposal_dist=proposal_distribution,\n",
    "    proposal_sampler=lambda x: proposal_distribution(x, proposal_width=1.0),\n",
    "    initial_value=initial_value,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.6, label='Sampled distribution')\n",
    "plt.plot(x, target_distribution(x), label='Target distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Metropolis-Hastings Sampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to conduct initial log likelihood evaluations\n",
    "def initial_data(t_init, traget_distn, prior_lower,prior_upper):\n",
    "    logliks = np.zeros(t_init-1)\n",
    "    thetas = np.zeros(t_init-1)\n",
    "    for i in range(t_init):\n",
    "        thetas[i] = np.random.uniform(lower=prior_lower,upper= prior_upper, size=1)\n",
    "        logliks[i] = target_distn(thetas[i])\n",
    "    return thetas, logliks\n",
    "\n",
    "def epsilon_gamma(m,v,u):\n",
    "    return norm.cdf(-np.abs(m-np.log(u))/np.sqrt(v))\n",
    "\n",
    "def post_preds(theta_prop, theta_prev, gpr,prior):\n",
    "    m,v=gpr.predict([theta_prev, theta_prop])\n",
    "    return m[1]-m[0] + np.log(prior(theta_prop)/prior(theta_prev)),v[0,0] + v[1,1] - 2*v[0,1]\n",
    "\n",
    "\n",
    "\n",
    "# we are assuming a prior which is uniform therefore we have upper and lower prior\n",
    "def GP_MH(N,t_init, target_distn, prior_lower,prior_upper,proposal,epsilon):\n",
    "    mh_samples=np.zeros(N)\n",
    "    thetas,logliks_init=initial_data(t_init, target_distn, prior_lower,prior_upper)\n",
    "    t = t_init-1\n",
    "    gpr = GPR()\n",
    "    gpr.fit(thetas,logliks_init)\n",
    "    for i in range(1,N+1):\n",
    "        theta_prop = proposal(mh_samples[i-1])\n",
    "        u = np.random.uniform(0,1)\n",
    "        # calculate post mean and post var\n",
    "        m,v=post_preds(theta_prop,mh_samples[i-1], gpr,prior)\n",
    "\n",
    "        while epsilon_gamma(m,v,u) > epsilon:\n",
    "            ''''\n",
    "                TODO: FIGURE OUT how to perform this optimisation\n",
    "                theta_star = np.argmax()\n",
    "                we are effectively using posterior predicitve and minimisng the variance of it \n",
    "            \n",
    "            '''\n",
    "            theta_star = 0\n",
    "            loglik_star=target_distn(theta_star)\n",
    "            # add to data\n",
    "            # refit GP\n",
    "            gpr=GPR()\n",
    "            gpr.fit(thetas,logliks_init)\n",
    "            # recalculate gamma_hat by predicting on thtea_prop and that prev sample\n",
    "            m,v=post_preds(theta_prop,mh_samples[i-1], gpr,prior)\n",
    "\n",
    "        # accept reject\n",
    "        if m>=np.log(u):\n",
    "            mh_samples[i]=theta_prop\n",
    "        else:\n",
    "            mh_samples[i]=mh_samples[i-1]\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
